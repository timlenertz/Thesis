\chapter{Implementation} \label{ch:implementation}
As part of this thesis, a relatively large software framework for working with point clouds was developed. Its main purpose was to have a toolkit based on which to process point clouds and experiment with different algorithms. It is not a finished product, as parts of it were changed or added depending on the requirements, but has developed into a sufficiently stable tool.

This chapter presents some of the work done, including software design choices, problems encountered, data structures and algorithms, and optimizations for handling large data sizes. It might be useful as a source of advice for similar software development projects.

\section{General architecture}
The software framework is called \texttt{pcf} (Point Cloud Fusion), and consists of three object-oriented libraries written in \cpp{}:
\begin{description}
\item[core] Main part containing implementation of the relevant objects, including point clouds with different data structures and point attributes, images, registration algorithms and geometrical data types.
\item[viewer] Visualization of the objects using OpenGL.
\item[experiment] System for running large number of \gls{icp} registrations with different parameters, and for generating visualizations of error metric functions, as shown in the previous chapter. For the \gls{icp} registrations, results are stored in an \emph{SQLite} database, from where different kinds of plots can be extracted.
\end{description}

\subsection{Programming language}
Point cloud algorithms work with large quantities of data. For example, the scan of the front facade of the HÃ´tel de Ville shown on figure \ref{fig:scan_005} in the previous chapter is a 202.2 MB file containing about $7$ million points. For any algorithms working on a per-point basis, minuscule execution time differences per point quickly multiply when applied on large point clouds. So it is important for code to be compiled into machine code instructions as efficiently as possible.

This makes \cpp{} a logical choice for the programming language. Its template meta-programming and function inlining facilities allow for object oriented code that expresses the operation to be performed in a semantically logical way to be reduced, at compile time, to the shortest possible set of instructions performing the same operations. For example the following code could sort a set of points \texttt{points} by increasing distance to a point \texttt{q}:
\begin{lstlisting}
sort(points.begin(), points.end(), bind(distance, q, _1))
\end{lstlisting}
This would instantiate a template function for \texttt{sort}, with a specific implementation of the sorting algorithm that uses the given iterators of the \texttt{points} set, the distance function\texttt{distance(q, p)}, and the swapping function for two points \texttt{swap(p1, p2)}. Both of these can then be inlined, removing the overhead of one of more function calls per point. Then the compiler could further optimize this specific instantiation of \texttt{sort}, for example by comparing the distances $| x^2 + y^2 + z^2 |$ instead of $\sqrt{x^2 + y^2 + z^2}$ because this would produce the same results, and saves one square root evaluation per point.\footnote{This particular optimization is probably not done by compilers because in some edge cases it can produce different results due to floating point imprecision.} 

In particular this is used in linear algebra libraries, where techniques such as \emph{expression templates} essentially allow for algebraic expressions involving scalars, vectors and matrices and others to be simplified at compile-time. For instance $k (\matr{A} + \matr{B})$, where $\matr{A}, \matr{B}$ are two large matrices, is computed more efficiently than $k \matr{A} + k \matr{B}$. For linear algebra, the \emph{Eigen} library is used in the project. It implements those optimizations, as well as SIMD vectorization. An operation on a 4-vector (for example a three-dimensional spatial vector in homogeneous coordinates), can be translated into a single SIMD instruction. It also provides support for 3D geometry, such as affine transformations, quaternions and planes.

Because point clouds processing often times involves executing the same operations for a large number of points, the algorithms are typically good candidates for parallelization. \emph{OpenMP} is used throughout the implementation to parallelize loops. However it is not part of standard \cpp{} and not supported by all compilers.

\subsection{Interactive console}
Most of \texttt{pcf} is implemented in the form of a library, as the goal was to have a toolkit providing the relevant functionality, and not just a single application. For applications such as running the \gls{icp} experiments, a corresponding executable program was written that uses the library.

But the compile-link-execute cycle makes this impracticable for interactively working with the point cloud objects, and in particular for visualizing the scene. Developing a textual or graphical user interface for the library would have been a large and useless workload, since eventually a user interface for every functionality of the library would have to be added.

A better approach would be to have an interactive console, similar to the ones that exist for scripting languages such as \emph{Python}, \emph{JavaScript} or \emph{Perl}. An attempt was made to add a \emph{Python} binding to the \cpp{} library using \emph{SWIG}, but it proved to create too make complications, in particular because of the intensive use of template meta-programming.

Instead the \emph{ROOT} framework is used, which provides the console and just-in-time compiler \emph{Cling} for \cpp{}. Despite some problems, it provides a practical way for using the API of \texttt{pcf} interactively. More details are described later in this chapter.



\section{Core}
As indicated, this main part of the implementation consists of template classes representing point clouds with different data structures, several algorithms to process points, a generic implementation of the \gls{icp} algorithm, a tree structure of space objects with poses defined relative to their parent object, virtual cameras, and other components.

\subsection{Point data type}
Two types of \emph{point} are provided. The most basic \verb%point_xyz% consists only the cartesian coordinates, and a flag indicating whether the point is \emph{valid}. It is represented using $4$ floating point values, and when the point is valid, the fourth component is set to $1.0$.

This way a 4-vector of the point's position in homogeneous coordinates is formed, which can be processed efficiently using SIMD instructions. With only three components point sets could be packed more densely in arrays, but because their addresses and sizes are not aligned on a power-of-two boundary, efficient processing using SIMD vectorization would not be possible.

The \emph{valid} flag is for example used in range images, where points in memory are arranged in a two-dimensional array, and some represent background pixels in image space. It also allows for removing points out of a point cloud without rearranging the other points or implementing a separate mask array.

Its subclass \verb%point_full% additionally contains an RGB color, a normal vector, a scalar weight and an integer index. It is padded to a size of $64$ bytes the nearest power of two. The index is used in some situations, for instance when corresponding points are ordered differently in memory for two point clouds, but should retain a reference to one another.

Different point types could also be useful, but are not implemented. Most parts of the framework are generalized to work with different point types.

\subsection{Point cloud data structures}
Even though a point cloud is defined as an unordered set of points, for it to be processed algorithmically the data needs to be laid out in a certain way in computer memory. The lack of an order relation between the points provides for great flexibility. At the same time, the structure of a point cloud is that of a sparse discrete set of points embedded in three-dimensional continuous space, whereas computer memory is a dense, discrete array of bits. Compromises need to be made in laying out the data in a way such that the required operations can be performed efficiently.

In the library different data structures are implemented as subclasses of a common abstract \verb%point_cloud% base class. The base class allocates and owns memory for the points, provides iterators to access them, including one which implicitly transforms their coordinates into another coordinate system. It also provides functions that do not modify the points, such as calculating the bounding box.

The subclasses handle the actual ordering of the points in memory, and implement algorithms such as closest point or $k$-nearest neighbor finding with it. Four subclasses are implemented: \emph{unorganized}, \emph{tree}, \emph{grid} and \emph{range}. Tough the subclasses may allocate additional memory to store information about the data structure, the points are always stored in an array ordered a certain way. This way any function which is agnostic about the point ordering can still access the point cloud the same way.

To simplify implementation, \verb%point_cloud% and its subclasses are themselves immutable objects: They can only be created once, but not reassigned, since this would require rebuilding the data structure. The points can still be modified. According to the programming by contract principle, the user of the classes needs to make sure not to modify the coordinates of points when they would make the points ordering invalid.

Putting a point cloud from one data structure into another is done via copy or move construction. In the latter case, the same memory is reused, reducing the total memory usage when working with large point clouds.


\subsubsection{Unorganized point cloud}
This class does not impose any ordering on the points, and is used for modifying it. For example, functions are provided to add noise, randomly displace the points, apply a transformation to the points, apply downsampling, and others.


\subsubsection{Tree point cloud}
Here the point clouds gets arranged into a space subdivision tree, such as a KdTree or an Octree. It is a generic implementation supporting different kinds of trees via a template argument. Points in memory are ordered such that points falling into same same tree node are stored in the same memory segment.

The tree point cloud allows for an efficient implementation of the closest point finding algorithm. To find the point closest to any position $\vec{x}$, it first searches only inside the leaf of the tree that $\vec{x}$ lies inside. If it doesn't find the closest point there, it also searches in the neighboring nodes. KdTrees make this most efficient, because there are only $2$ child nodes per parent, and the tree depth is balanced. There can be edge cases where the closest point actually lies in a node that is several layers in the tree away from the one containing $\vec{x}$. Therefore the algorithm can be accelerated by specifying a \emph{accept distance} $a > 0$ and a \emph{reject distance} $r < \infty$. When any point closer than $a$ is found it is accepted as closest point, and when it is determined that no point can lie closer than $r$, the algorithm fails.

There are other approaches for optimizing closest point finding on tree structures, such as the one presented in \cite{Kim2009} for Octrees.

\subsubsection{Grid point cloud}
For the grid point cloud the points instead divided into a regular grid of equal-sized, axis-aligned cubic cells. Using this an efficient $k$-nearest neighbor algorithm was implemented, based on the approach described in \cite{Sank2007}. Its execution time gets shorter the smaller the cubic cells are set. But this also entails a cubical increase in memory usage, because a separate list of the memory offsets for each cell is kept. 

\subsubsection{Range point cloud}
Represents a range image. Points are ordered row-by-row like the image pixels. Invalid points are put in for background pixels of the range image. The points are always set in the coordinate system where the camera is at origin.

A subclass of it the \emph{camera range point cloud}, also includes the virtual camera object by which the points were projected. When artificial range image are generated, a point cloud of that class is returned. It provides for example a function to back-project any position in space to the corresponding position in image space.

\subsubsection{Const-correctness}
Properly implementing const-correctness on the point cloud classes can be a difficult problem. The point cloud are container classes which \emph{own} their points, and thus \emph{const} access to the point cloud should only allow \emph{const} to its points. This would for example require two instances of the \verb&closest_point& algorithm: A \emph{const} version which also returns a reference to a \emph{const} point, and a corresponding \emph{non-const} version with exactly the same code.

The underlying problem is \cpp{}'s lack of support for templatizing member functions for \emph{const}-ness of \verb&this&. Proposals exist to add this into future versions of the \cpp{} standard. Some solution possibilities are (1) to implement the algorithm in a non-member function which takes the \emph{const} or \emph{non-const} class type as template argument, and then have the two member functions call this algorithm, (2) to implement only one version of the member function, and have the other one forward the call to it using \verb&const_cast&, or finally (3) to outsource algorithms into non-owning helper classes.

This last approach just moves the problem somewhere else, as there would have to be for example a \emph{const} and a \emph{non-const} \verb&point_cloud_segment& class.


\subsection{Space object}
The point clouds, as well as other classes such as geometrical objects or cameras, are subclasses of \verb&space_object&, which contains a pose, and references a parent \verb&space_object&. Each space object thus defines a coordinate system.

Points in a point cloud contain the coordinates in its own coordinate system. The unorganized point cloud provides a function to \emph{apply} the pose: It sets the space object pose to identity, but changes the points' coordinates to the equivalent positions. This can not be done in organized (i.e. tree, grid, range) point clouds, because altering the point positions would make the data structure invalid.

The point cloud base class also provides a \emph{transform iterator}, which iterates through the points, but instead of returning the actual point, return a transformed version of it. This way it is possible to use a construct like
\begin{lstlisting}
for(auto it = P.begin_relative_to(Q); it != P.end_relative_to(); ++it) {
	const point_full p = *P;
	const point_full& q = Q.closest_point(p); 
	cout << "closest point distance = " << distance(p, q) << endl;
}
\end{lstlisting}
This iterates through the points of $P$ using a transform iteration which transforms them into the coordinate system of $Q$. Then the closest point distance to a point in $Q$ is measured. \verb&begin_relative_to& traverses the space object tree to calculate the appropriate rigid transformation. The point cloud functions like \verb&closest_point& expect a point or position in the same coordinate system as the point cloud as input.

When transforming a point, both its position coordinates, and its normal vector coordinates need to be changed. Only the linear part of the transformation is applied to the normal vector. The point classes are not subclasses of \verb&space_object&. A good alternate approach may be to introduce a wrapper class which associates a space object pose to a point, so that points are automatically put in the right coordinate system as needed, simplifying usage. This is not implemented, and it would have to be designed in such a way that any additional overhead gets resolved at compile time.

\subsection{Point algorithms}
Several algorithm operating on point, or on point clouds are implemented, for example the curvature measure developed in chapter \ref{ch:large_model}, or the creation of a sample point cloud $Q$ by adding points on tangent planes around the existing points of $P$.

The algorithms are separated into two categories: Some which operate on any set of points, and some which operate on a point cloud, and make use of the point cloud's member functions such as its \verb&closest_point& implementation. Similar to the \gls{stl} \verb&<algorithm>& header, they are implemented as standalone template functions and not as member functions of the point class. 

The reason is that \emph{point} algorithms should be able to work with any list of points, not only points in a \verb&point_cloud&. This includes for example points yielded by the transform iterator. Like the \gld{stl} algorithms, they take two generic iterators as input which delimit the range. \emph{Point cloud} algorithms take a generic point cloud subclass as input.


\subsection{Iterative Correspondences Registration}
A generic implementation the \gls{icp} framework algorithm was created in form of a template class taking as template arguments the loose and fixed point clouds types, a \emph{correspondences} class, an \emph{error metric} class, and a \emph{transformation estimation} class. Only point-to-point \gls{icp} is implemented.

The \emph{correspondences} classes output the found correspondence pairs to a generic \emph{receiver} callback object given as function argument. When parallelization is used, it also creates new \emph{receiver} instances, sends correspondence pairs to them on different threads, and then joins them together.

The \emph{iterative correspondences registration} class itself uses a \emph{receiver} class that forwards the incoming correspondence pairs to both a \emph{error metric} and a \emph{transformation estimation} instance.

It can run the \gls{icp} registration for a fixed number of iterations, or until the error metric has reaches a predefined minimal value. Because it takes only \emph{const} references to the loose and fixed point clouds, it does not modify the loose point cloud's pose. Instead, the accumulated transformation is stored which can then be applied to the point cloud by the class user.

Having the class modify the point cloud would for example make it impossible to run several registration attempts on it simultaneously, or to use registration on a \emph{const} point cloud.


\subsection{Other components}
Import and export of point clouds from and to the PLY file format is implemented. This is a de factor standard file format for point cloud data and is also supported by most other point cloud processing software. The format is versatile, and can also be used to store meshes, triangular faces, various kinds of point attributes, and even data other than 3D models.

The framework implements a custom memory allocator for the point cloud class. When the requested size is higher than a fixed threshold (set for example at 1 GB), it does not allocate member using the standard \verb&malloc& call, but instead creates a temporary memory-mapped file on the disk, and allocates a virtual memory region which maps to it. This operating system-specific feature is implemented for both Windows and Darwin. Whether it provides an advantage depends on the operating system, but it typically alters the virtual memory pagination strategy such that it no longer tries to load as many pages as possible into physical memory. On Darwin, this would cause the system to stall as pages get swapped out when allocating large memory chunks, leading eventually to a segmentation fault. With memory-mapped files this issue was resolved.


\section{Viewer}
The viewer is also implemented in form of a library, and can be controlled interactively from the \emph{Cling} console. It provides a \emph{scene} with \emph{scene objects}, which can be moved in space either programmatically or using the keyboard and mouse on the visualized. \emph{Scene objects} are tied to \emph{space object} like point clouds, and hook functions are installed so that when the \emph{space object}'s absolute pose is changed (which can also happen when its parent's pose changes), the \emph{scene object} gets also updated.

\emph{Scene objects} implement rendering of a visualization of the object in an active OpenGL context. They are implemented for point clouds, some geometric objects, and correspondences.

\subsection{Point cloud visualization}
Copying the entire set of points into GPU and rendering it using \texttt{glDrawArrays} would cease to work for large point clouds. A system which dynamically reduces the number of points sent to OpenGL as needed.

An algorithm is implemented which dynamically extracts a visible subset of the point cloud in function of the camera pose and field of view, using both viewing frustum culling and live downsampling. The \emph{scene point cloud} internally stores a copy of the point cloud in an as an Octree \emph{tree point cloud}. The storage order points in the Octree leafs is randomly shuffled.

\subsubsection{Viewing frustum culling}
Each node of the Octree corresponds to a cubic region in space, an can be completely outside, completely inside, or partially inside the camera viewing frustum. The algorithm takes only the points lying in nodes that are completely inside the viewing frustum. For nodes that are partially inside, it recursively traverses its child nodes, and takes points from child nodes that are completely inside, and also leafs that are partially inside. This is necessary because leafs are not further subdivided.

\subsubsection{Real-time down-sampling}
If the number of points in this subset $P'$ if greater than a predefined rendered capacity $c$, it is down-sampled in real-time. A simple random down-sampling is applied, in which each point is chosen with a probability of $\frac{c}{\| P' \|}$. Simply taking the $c$ first (or last) points in the array $P'$ would include more points from the first included node, and none from the later ones. But the chosen points should be evenly distributed among the nodes.

An algorithm which runs the random number generator could be inefficient, depending on the pseudo-random number algorithm used. Also it would not guarantee that exactly $c$ points are selected.

If $\| P' \|$ is dividable by $c$, then a good deterministic approach is to select every $k$-th point, where $k = \frac{\| P' \|}{c}$. If this not the case, a method akin to \emph{Bresenham's line algorithm} is used to select point indices between $1$ and $\|P'\|$. This is efficient, deterministic, and choses points from the array at an approximatively uniform distribution.

The reason that points in the leaf nodes were shuffled is to remove and spatial bias resulting from the way points were ordered previously in the original point cloud, from which this \emph{scene point cloud} was constructed. For example when Bresenham's line algorithm selects two neighboring points in the array, their spatial positions should not be closer than for any other two points.

\subsubsection{Buffer swapping}
A dual-buffered method is used to apply this dynamic down-sampling in the renderer. Two \gls{gpu} buffers of capacity $c$ are allocated. The first one contains the array of points currently visible onscreen, and is passed to the OpenGL pipeline at each frame using a \verb&glDrawArrays& call.

The dynamic down-sampling algorithm is run in a separate thread, whenever the camera is moved. It outputs the selected point into the second \gls{GPU} buffer. When it has finished, the two buffers are \emph{swapped}, meaning they change roles, without any copying taking place. This way blocking is minimized.


\section{Console}
As described, the \emph{ROOT} framework with the \emph{Cling} just-in-time \cpp{} compiler was used to have an interactive console to access the library.

\emph{ROOT Data Analysis Framework} is a \cpp{} framework providing functionality to handle and analyze large amounts of scientific data. It contains components for recording, analyzing and visualizing two-dimensional and three-dimensional physical fields, function plots, histograms, images, and more. The framework is available for free download under the \emph{LGPL} license. It is developed by \emph{CERN}, and was initially created in the context of a particle physics experiment in 1997.

The framework is built so that \cpp{} can be used interactively is a console interface similar to \emph{iPython} for example. The core components for this is the included \emph{Cling} just-in-time compiler, which a fork of the open source \emph{LLVM/Clang} \cpp{} compiler. \emph{Cling} provides a console, which adds some functionality to \cpp{} such as the ability to dynamically instantiate objects in a new \emph{console} scope. \emph{ROOT} also provides a large set of graphical user interface components, which all respond dynamically to input from the console. For example the histograms in chapter \ref{ch:large_model} were created using this.

\subsection{Usage with framework}
Importantly, \emph{Cling} is able to load external \cpp{} headers, and link to prebuilt dynamic libraries. Being based on \emph{Clang}, it has full support for \cpp{}. Linking to the \texttt{pcf} framework from \emph{Cling} works well, even if this was done in an afterthought. For example, the following code can be entered line by line in the console:
\begin{lstlisting}
using namespace pcf;

// Create two point cloud object of the Bunny model
unorganized_point_cloud_full P = import_point_cloud("bunny.ply");
kdtree_point_cloud_full Q = P; // store Q in KdTree for efficient closest point finding

// P = loose point cloud.
// Make point colors white, randomly displace points, and randomly down-sample to 30%
set_unique_color(P.begin(), P.end(), rgb_color::white);
P.randomly_displace_points(std::uniform_real_distribution<float>(-0.002, +0.002));
P.downsample_random(0.3);

// Q = fixed point cloud
// Make point colors red, and apply some translation and rotation
set_unique_color(Q.begin(), Q.end(), rgb_color::red);
Q.move_x(0.01);
Q.rotate_y_axis(angle::degrees(0.1));

// Register P and Q using ICP, selecting all points from P, and run for 10 iterations
auto reg = make_iterative_closest_point_registration(Q, P, accept_point_filter());
reg.maximal_iterations = 10;
reg.run();

// Create OpenGL visualizer, and display both P and Q
viewer_window vw;
vw->add(P, Q);

// Now apply the estimated transformation to P
// The visualizer responds interactively and shows P with its new pose
P.set_relative_pose(reg.accumulated_transformation());
\end{lstlisting}


\subsection{Problems}
However, some problems were encountered when using \texttt{pcf} with \emph{Cling}, including:
\begin{itemize}
\item Each time the console is started, it needs to link the dynamic library and parse headers. The whole process usually takes about $4$ seconds.
\item Due to bugs in the current version of \emph{Cling}, entering syntax errors into the console sometimes messes up the internal state of \emph{Cling}, so that subsequent commands no longer work properly. The console needs to be relaunched.
\item Some \cpp{} constructs are unsupported by \emph{Cling} and produce error messages.
\item Objects dynamically created on the global \emph{console} scope cannot be deleted. The \emph{ROOT} framework was instead designed to allocate all objects on the heap and use smart pointers.
\item The \emph{Cling} compiler does not support OpenMP.
\item Most importantly, the current version of \emph{Cling} compiled code without optimizations. Because \texttt{pcf} is heavily template-based, most of its functions are \gls{jit} compiled by \emph{Cling} and not precompiled in the dynamic library. This makes most algorithms much less efficient, especially when working with large data sets. To solve this, some template functions and classes are explicitly instantiated in \texttt{pcf}, so that frequently used versions of it are compiled with optimizations and included into the dynamic library when building it.
\end{itemize}